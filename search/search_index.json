{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KAROLOS KAROLOS (Open-Source Robot-Task Learning Simulation) is an open-source simulation and reinforcement learning suite. KAROLOS was developed with a focus on: scalability : As reinforcement learning algorithms require significant amounts of experience, KAROLOS enables the parallelization of environments. This way, you spend less time on data collection and more time on training and prototyping. modularization : More and more research in reinforcement learning is looking into the transfer of agents from one environment to another. KAROLOS was developed to quickly generate environments with different robot-task combinations. KAROLOS is still under active development. We encourage you check back regularly.","title":"Home"},{"location":"#karolos","text":"KAROLOS (Open-Source Robot-Task Learning Simulation) is an open-source simulation and reinforcement learning suite. KAROLOS was developed with a focus on: scalability : As reinforcement learning algorithms require significant amounts of experience, KAROLOS enables the parallelization of environments. This way, you spend less time on data collection and more time on training and prototyping. modularization : More and more research in reinforcement learning is looking into the transfer of agents from one environment to another. KAROLOS was developed to quickly generate environments with different robot-task combinations. KAROLOS is still under active development. We encourage you check back regularly.","title":"KAROLOS"},{"location":"agents/","text":"Agents todo: describe general functionality (more detailed than architecture), link papers / openai-spinningup lots of links SAC DDPG","title":"Agents"},{"location":"agents/#agents","text":"todo: describe general functionality (more detailed than architecture), link papers / openai-spinningup lots of links","title":"Agents"},{"location":"agents/#sac","text":"","title":"SAC"},{"location":"agents/#ddpg","text":"","title":"DDPG"},{"location":"architecture/","text":"Architecture How components work together, mostly interfaces, no inner workings Set up an experiment by modifying the training config in trainer.py and starting trainer.py. During the initialization of each experiment a folder for saving tensorboard, models and config files is created in a results folder at the project root level. It is also possible to reload previous experiments by passing the base_experiment key in the config. The environment orchestrator handles the creation and communication with the number_envs environments by spawning each environment in a process using the native multiprocessing package of python. Environments (currently environment_robot_task.py ) are composed of a robot and a task which the user can combine in arbitrary ways. During initialization the environments return the observation and action space to the trainer which initializes the agent using these. Additionally the success criterion and the reward function are passed back to the trainer from the respective task. The success criterion is used to determine the test_success_ratio which is the number of concluded test runs with success_criterion=True compared to the total number of tests for the current test period. If the previous best ratio is outperformed, the agent is saved to the model directory. Success criterion- and reward function are set in the specific task file. This file in junction with the task meta class specifies everything except for the communication with the simulated robot. Task objects and robot files are saved in urdf format in the data folder and read by pybullet which composes the simulation environment. Agent files are stored in agents folder, test scripts are also included.","title":"Architecture"},{"location":"architecture/#architecture","text":"How components work together, mostly interfaces, no inner workings Set up an experiment by modifying the training config in trainer.py and starting trainer.py. During the initialization of each experiment a folder for saving tensorboard, models and config files is created in a results folder at the project root level. It is also possible to reload previous experiments by passing the base_experiment key in the config. The environment orchestrator handles the creation and communication with the number_envs environments by spawning each environment in a process using the native multiprocessing package of python. Environments (currently environment_robot_task.py ) are composed of a robot and a task which the user can combine in arbitrary ways. During initialization the environments return the observation and action space to the trainer which initializes the agent using these. Additionally the success criterion and the reward function are passed back to the trainer from the respective task. The success criterion is used to determine the test_success_ratio which is the number of concluded test runs with success_criterion=True compared to the total number of tests for the current test period. If the previous best ratio is outperformed, the agent is saved to the model directory. Success criterion- and reward function are set in the specific task file. This file in junction with the task meta class specifies everything except for the communication with the simulated robot. Task objects and robot files are saved in urdf format in the data folder and read by pybullet which composes the simulation environment. Agent files are stored in agents folder, test scripts are also included.","title":"Architecture"},{"location":"contributing/","text":"Contibuting Contribute by implementing new functionalities, robots or tasks. Test your new feature with some task/robot combination (reach has the least amount of stress per unit of test ;-) ) and include a converging run with your merge request. If you implement a new task, also discuss your used success_criterion and reward function, we would like to avoid task specific reward shaping. Robots Added custom robots should use the same functions as panda.py and ur5.py. Pybullet allows for position, torque and velocity control of the robot. Torque and position control have been tested and work on the current architecture. robot.init - read config and parameters; load robot urdf files; define joints, links, observation and action space; robot.reset - reset the robot until no contact points with itself are left robot.step - execute action and return observation by get_observation() robot.get_observation - return tcp position, link velocities and posiitons. Strucutre is defined in robot.observation_space robot.randomize - Domain Randomization (DR) using config robot.standardize - Reset DR to standard case for testing Tasks Custom tasks also have to reuse the same structure as e.g. the reach task and furthermore inherit from the Base Task class. Try to first achieve convergence using a non-specific success criterion and reward function (e.g. exponential decreasing distance measure) and gradually introduce more task-specific reward shaping. If you need new objects like doors, tables or more, visit the urdf forums for available meshes. task.init - set gravity, cartesian offset, DR, limits and observation/goal space load urdf files (-objects) for the task task.reset - reset the objects/targets until no contact points with robot task.get_observation -","title":"Contributing"},{"location":"contributing/#contibuting","text":"Contribute by implementing new functionalities, robots or tasks. Test your new feature with some task/robot combination (reach has the least amount of stress per unit of test ;-) ) and include a converging run with your merge request. If you implement a new task, also discuss your used success_criterion and reward function, we would like to avoid task specific reward shaping.","title":"Contibuting"},{"location":"contributing/#robots","text":"Added custom robots should use the same functions as panda.py and ur5.py. Pybullet allows for position, torque and velocity control of the robot. Torque and position control have been tested and work on the current architecture. robot.init - read config and parameters; load robot urdf files; define joints, links, observation and action space; robot.reset - reset the robot until no contact points with itself are left robot.step - execute action and return observation by get_observation() robot.get_observation - return tcp position, link velocities and posiitons. Strucutre is defined in robot.observation_space robot.randomize - Domain Randomization (DR) using config robot.standardize - Reset DR to standard case for testing","title":"Robots"},{"location":"contributing/#tasks","text":"Custom tasks also have to reuse the same structure as e.g. the reach task and furthermore inherit from the Base Task class. Try to first achieve convergence using a non-specific success criterion and reward function (e.g. exponential decreasing distance measure) and gradually introduce more task-specific reward shaping. If you need new objects like doors, tables or more, visit the urdf forums for available meshes. task.init - set gravity, cartesian offset, DR, limits and observation/goal space load urdf files (-objects) for the task task.reset - reset the objects/targets until no contact points with robot task.get_observation -","title":"Tasks"},{"location":"environments/","text":"Environments todo: describe general functionality (more detailed than architecture), link papers / openai-spinningup lots of links SAC DDPG","title":"Environments"},{"location":"environments/#environments","text":"todo: describe general functionality (more detailed than architecture), link papers / openai-spinningup lots of links","title":"Environments"},{"location":"environments/#sac","text":"","title":"SAC"},{"location":"environments/#ddpg","text":"","title":"DDPG"},{"location":"examples/","text":"Examples running experiments best practices - choosing cpus","title":"Examples"},{"location":"examples/#examples","text":"running experiments best practices - choosing cpus","title":"Examples"},{"location":"getting_started/","text":"Getting Started First steps Run an experiment by launching a manager python manager/manager.py You can monitor the progress of your experiment in real-time with a tensorboard tensorboard --logdir results Experiment Configuration An experiment is parametrized by a configuration dictionary. { \"total_timesteps\": 1_000_000, # experiment is run until this amount of experience samples is collected \"test_interval\": 10_000, # samples after which the agent performance is tested \"number_tests\": 100, # amount of episodes to be run for a test \"her_ratio\": 0., # amount of data to be generate with hindsight experience replay relative to amount of collected samples. The generated data is added on top of the collected data. \"number_processes\": 1, # amount of parallel processes in which environments are launched \"number_threads\": 1, # amount of environments launched per process \"agent_config\": {...}, # agent configuration, see seperate subchapter \"env_config\": {...} # environment configuration, see seperate subchapter } The agent configuration agent_config is specific to the agent you want to use. Please refer to the chapter Agents for specifics. Likewise, the environment configuration agent_config is specific to the environment you want to use. Please refer to the chapter Environments for specifics. This framework uses Pytorch applied to Pybullet simulation environments composed of a robot and a task which can be arbitrarily combined. Run the minimal configuration with an algorithm and task/robot combination of your choice. Soft-Actor-Critic with reach task on pandas robot should converge after about 5 million steps to a successful policy. See architecture doc for further description of the components and contributing if you'd like to contribute a feature, robot, task or new agent.","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"","title":"Getting Started"},{"location":"getting_started/#first-steps","text":"Run an experiment by launching a manager python manager/manager.py You can monitor the progress of your experiment in real-time with a tensorboard tensorboard --logdir results","title":"First steps"},{"location":"getting_started/#experiment-configuration","text":"An experiment is parametrized by a configuration dictionary. { \"total_timesteps\": 1_000_000, # experiment is run until this amount of experience samples is collected \"test_interval\": 10_000, # samples after which the agent performance is tested \"number_tests\": 100, # amount of episodes to be run for a test \"her_ratio\": 0., # amount of data to be generate with hindsight experience replay relative to amount of collected samples. The generated data is added on top of the collected data. \"number_processes\": 1, # amount of parallel processes in which environments are launched \"number_threads\": 1, # amount of environments launched per process \"agent_config\": {...}, # agent configuration, see seperate subchapter \"env_config\": {...} # environment configuration, see seperate subchapter } The agent configuration agent_config is specific to the agent you want to use. Please refer to the chapter Agents for specifics. Likewise, the environment configuration agent_config is specific to the environment you want to use. Please refer to the chapter Environments for specifics. This framework uses Pytorch applied to Pybullet simulation environments composed of a robot and a task which can be arbitrarily combined. Run the minimal configuration with an algorithm and task/robot combination of your choice. Soft-Actor-Critic with reach task on pandas robot should converge after about 5 million steps to a successful policy. See architecture doc for further description of the components and contributing if you'd like to contribute a feature, robot, task or new agent.","title":"Experiment Configuration"},{"location":"installation/","text":"Installation First, clone the repository git clone https://github.com/tmdt-buw/karolos cd karolos Install the dependencies using conda (recommended option) conda env create -f environment.yml conda activate karolos If you have CUDA installed, you will need to install the package cudatoolkit using the version which fits your system.","title":"Installation"},{"location":"installation/#installation","text":"First, clone the repository git clone https://github.com/tmdt-buw/karolos cd karolos Install the dependencies using conda (recommended option) conda env create -f environment.yml conda activate karolos If you have CUDA installed, you will need to install the package cudatoolkit using the version which fits your system.","title":"Installation"},{"location":"robots/","text":"Robots Panda UR5","title":"Robots"},{"location":"robots/#robots","text":"","title":"Robots"},{"location":"robots/#panda","text":"","title":"Panda"},{"location":"robots/#ur5","text":"","title":"UR5"},{"location":"tasks/","text":"Tasks Reach Pick_Place Push","title":"Tasks"},{"location":"tasks/#tasks","text":"","title":"Tasks"},{"location":"tasks/#reach","text":"","title":"Reach"},{"location":"tasks/#pick_place","text":"","title":"Pick_Place"},{"location":"tasks/#push","text":"","title":"Push"}]}